{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ced762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2eff1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you have multiple HCS files to process? (yes/no): yes\n",
      "Please enter the path to an HCS file (html/xls/csv) (or 'done' to finish): today2.xlsx\n",
      "Please enter the path to an HCS file (html/xls/csv) (or 'done' to finish): done\n",
      "✅ Successfully processed: today2.xlsx\n",
      "\n",
      "✅ Combined 1 HCS files\n",
      "First few rows of combined data:\n",
      "   ISSTYPE       CARD_NUMBER             CRDH_NAME         ATM_ACCT  \\\n",
      "0  REPLACE  4753960000746392           IJAJ HUSSEN  414701406417018   \n",
      "1  REPLACE  4753960000742094  TEJLAXMI RAJBHANDARI  210200053905015   \n",
      "2  REPLACE  4753960000742102  PURNA BAHADUR KHADGI  210201108720016   \n",
      "3  REPLACE  4753960000742193    SUKRA P. RANJITKAR  210001312381019   \n",
      "4  REPLACE  4753960000742219        BISHNU B SINGH  726105035139015   \n",
      "\n",
      "     ISS_DATE  EXPIR_DATE  CARD_ID  \n",
      "0  2025-03-24  2029-03-24  1308014  \n",
      "1  2025-03-24  2029-03-24  1306670  \n",
      "2  2025-03-24  2029-03-24  1306671  \n",
      "3  2025-03-24  2029-03-24  1306678  \n",
      "4  2025-03-24  2029-03-24  1306680  \n",
      "Would you like to save the combined data to a CSV file? (yes/no): no\n"
     ]
    }
   ],
   "source": [
    "# Update of multiple hcs file\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ask user if they have multiple HCS files\n",
    "multiple_files = input(\"Do you have multiple HCS files to process? (yes/no): \").lower().strip()\n",
    "\n",
    "all_hcs_data = []\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Process a single file (CSV, XLS, or HTML) and return a DataFrame.\"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith(('.xls', '.xlsx')):\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif file_path.endswith('.html'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "            df_list = pd.read_html(html_content)\n",
    "            df = df_list[0]\n",
    "            # Set first row as column names for HTML files\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df[1:].reset_index(drop=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use CSV, XLS/XLSX, or HTML.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if multiple_files == 'yes':\n",
    "    # Ask for multiple file paths\n",
    "    hcs_paths = []\n",
    "    while True:\n",
    "        path = input(\"Please enter the path to an HCS file (html/xls/csv) (or 'done' to finish): \")\n",
    "        if path.lower() == 'done':\n",
    "            break\n",
    "        if os.path.exists(path):\n",
    "            hcs_paths.append(path)\n",
    "        else:\n",
    "            print(\"Error: The file does not exist. Please check the path and try again.\")\n",
    "\n",
    "    if not hcs_paths:\n",
    "        print(\"Error: No valid files provided. Exiting.\")\n",
    "    else:\n",
    "        # Process each HCS file\n",
    "        for hcs_path in hcs_paths:\n",
    "            hcs = process_file(hcs_path)\n",
    "            if hcs is not None:\n",
    "                all_hcs_data.append(hcs)\n",
    "                print(f\"✅ Successfully processed: {hcs_path}\")\n",
    "\n",
    "        # Combine all dataframes\n",
    "        if all_hcs_data:\n",
    "            combined_hcs = pd.concat(all_hcs_data, ignore_index=True)\n",
    "            print(f\"\\n✅ Combined {len(all_hcs_data)} HCS files\")\n",
    "            print(\"First few rows of combined data:\")\n",
    "            print(combined_hcs.head())\n",
    "            # Option to save\n",
    "            save = input(\"Would you like to save the combined data to a CSV file? (yes/no): \").lower().strip()\n",
    "            if save == 'yes':\n",
    "                output_path = input(\"Enter the output file path (e.g., output.csv): \")\n",
    "                combined_hcs.to_csv(output_path, index=False)\n",
    "                print(f\"✅ Saved to {output_path}\")\n",
    "        else:\n",
    "            print(\"No data was successfully processed.\")\n",
    "\n",
    "else:\n",
    "    # Single file processing\n",
    "    hcs_path = input(\"Please enter the path to your HCS file (html/xls/csv): \")\n",
    "    if not os.path.exists(hcs_path):\n",
    "        print(\"Error: The file does not exist. Please check the path and try again.\")\n",
    "    else:\n",
    "        hcs = process_file(hcs_path)\n",
    "        if hcs is not None:\n",
    "            print(f\"\\n✅ Successfully processed: {hcs_path}\")\n",
    "            print(\"First few rows of data:\")\n",
    "            print(hcs.head())\n",
    "            print(f\"Total rows: {len(hcs)}\")\n",
    "            # Option to save\n",
    "            save = input(\"Would you like to save the data to a CSV file? (yes/no): \").lower().strip()\n",
    "            if save == 'yes':\n",
    "                output_path = input(\"Enter the output file path (e.g., output.csv): \")\n",
    "                hcs.to_csv(output_path, index=False)\n",
    "                print(f\"✅ Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999445f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you have multiple text files for name extraction? (yes/no):  yes\n",
      "Please enter the path to a text file (or 'done' to finish):  today\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File does not exist. Please check the path and try again.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the path to a text file (or 'done' to finish):  today.txt\n",
      "Please enter the path to a text file (or 'done' to finish):  today1.txt\n",
      "Please enter the path to a text file (or 'done' to finish):  today2.txt\n",
      "Please enter the path to a text file (or 'done' to finish):  today3.txt\n",
      "Please enter the path to a text file (or 'done' to finish):  done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed: today.txt\n",
      "✅ Successfully processed: today1.txt\n",
      "✅ Successfully processed: today2.txt\n",
      "✅ Successfully processed: today3.txt\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you have multiple HCS files to process? (yes/no):  no\n",
      "Please enter the path to your HCS file (html/xls/csv):  hcs.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed: hcs.xlsx\n",
      "\n",
      "Original HCS rows: 237\n",
      "Extracted names: 1473\n",
      "Final matched rows: 234\n",
      "\n",
      "First few rows of final DataFrame:\n",
      "        EXTRACTED_NAME  ISSTYPE   CARD_NUMBER           CRDH_NAME  \\\n",
      "16    PRAKASH B ROKAYA  REPLACE  4.753960e+15    PRAKASH B ROKAYA   \n",
      "17     PAN SINGH DHAMI  REPLACE  4.753960e+15     PAN SINGH DHAMI   \n",
      "24  NAR BAHADUR BOHARA  REPLACE  4.753960e+15  NAR BAHADUR BOHARA   \n",
      "33         SABIN DAHAL  REPLACE  4.753960e+15         SABIN DAHAL   \n",
      "35   SUSHMA P ADHIKARI  REPLACE  4.753960e+15   SUSHMA P ADHIKARI   \n",
      "\n",
      "        ATM_ACCT    ISS_DATE  EXPIR_DATE    CARD_ID MATCHED_EXTRACTED_NAME  \n",
      "16  8.234051e+14  2025-03-24  2029-03-24  1307676.0       PRAKASH B ROKAYA  \n",
      "17  8.234017e+14  2025-03-24  2029-03-24  1307677.0        PAN SINGH DHAMI  \n",
      "24  8.234016e+14  2025-03-24  2029-03-24  1307684.0     NAR BAHADUR BOHARA  \n",
      "33  2.126052e+14  2025-03-24  2029-03-24  1307693.0            SABIN DAHAL  \n",
      "35  4.153054e+14  2025-03-24  2029-03-24  1307717.0      SUSHMA P ADHIKARI  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to save the results to a CSV file? (yes/no):  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results not saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize a name by converting to uppercase, removing extra spaces, and splitting into components.\"\"\"\n",
    "    if pd.isna(name) or name == \"NaN\":\n",
    "        return {\"first\": \"\", \"middle\": \"\", \"surname\": \"\"}\n",
    "    # Remove extra spaces and convert to uppercase\n",
    "    name = \" \".join(name.split()).upper()\n",
    "    parts = name.split()\n",
    "    \n",
    "    if len(parts) == 1:\n",
    "        return {\"first\": parts[0], \"middle\": \"\", \"surname\": \"\"}\n",
    "    elif len(parts) == 2:\n",
    "        return {\"first\": parts[0], \"middle\": \"\", \"surname\": parts[1]}\n",
    "    else:\n",
    "        return {\"first\": parts[0], \"middle\": \" \".join(parts[1:-1]), \"surname\": parts[-1]}\n",
    "\n",
    "def process_hcs_file(file_path):\n",
    "    \"\"\"Process an HCS file (HTML, CSV, or XLS/XLSX) and return a DataFrame.\"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith(('.xls', '.xlsx')):\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif file_path.endswith('.html'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "            df_list = pd.read_html(html_content)\n",
    "            df = df_list[0]\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df[1:].reset_index(drop=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use CSV, XLS/XLSX, or HTML.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# --- Process Extracted Names ---\n",
    "multiple_txt_files = input(\"Do you have multiple text files for name extraction? (yes/no): \").lower().strip()\n",
    "all_extracted_names = []\n",
    "\n",
    "if multiple_txt_files == 'yes':\n",
    "    txt_paths = []\n",
    "    while True:\n",
    "        path = input(\"Please enter the path to a text file (or 'done' to finish): \")\n",
    "        if path.lower() == 'done':\n",
    "            break\n",
    "        if os.path.exists(path):\n",
    "            txt_paths.append(path)\n",
    "        else:\n",
    "            print(\"Error: File does not exist. Please check the path and try again.\")\n",
    "\n",
    "    if not txt_paths:\n",
    "        print(\"Error: No valid text files provided. Exiting.\")\n",
    "    else:\n",
    "        for txt_path in txt_paths:\n",
    "            try:\n",
    "                with open(txt_path, 'r', encoding='utf-8') as txt_file:\n",
    "                    for line in txt_file:\n",
    "                        matches = re.findall(r'NPR\\s{0,4}([A-Z][A-Z.\\s]+)', line)\n",
    "                        if matches:\n",
    "                            for match in matches:\n",
    "                                all_extracted_names.append(match.strip())\n",
    "                        else:\n",
    "                            all_extracted_names.append(\"NaN\")\n",
    "                print(f\"✅ Successfully processed: {txt_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error processing {txt_path}: {str(e)}\")\n",
    "else:\n",
    "    txt_path = input(\"Please enter the path to your text file: \")\n",
    "    if not os.path.exists(txt_path):\n",
    "        print(\"Error: File does not exist. Please check the path and try again.\")\n",
    "    else:\n",
    "        try:\n",
    "            with open(txt_path, 'r', encoding='utf-8') as txt_file:\n",
    "                for line in txt_file:\n",
    "                    matches = re.findall(r'NPR\\s{0,4}([A-Z][A-Z.\\s]+)', line)\n",
    "                    if matches:\n",
    "                        for match in matches:\n",
    "                            all_extracted_names.append(match.strip())\n",
    "                    else:\n",
    "                        all_extracted_names.append(\"NaN\")\n",
    "            print(f\"✅ Successfully processed: {txt_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {txt_path}: {str(e)}\")\n",
    "\n",
    "if not all_extracted_names:\n",
    "    print(\"Error: No names extracted. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "extracted_df = pd.DataFrame({'EXTRACTED_NAME': all_extracted_names})\n",
    "\n",
    "# --- Process HCS Files ---\n",
    "multiple_hcs_files = input(\"Do you have multiple HCS files to process? (yes/no): \").lower().strip()\n",
    "all_hcs_data = []\n",
    "\n",
    "if multiple_hcs_files == 'yes':\n",
    "    hcs_paths = []\n",
    "    while True:\n",
    "        path = input(\"Please enter the path to an HCS file (html/xls/csv) (or 'done' to finish): \")\n",
    "        if path.lower() == 'done':\n",
    "            break\n",
    "        if os.path.exists(path):\n",
    "            hcs_paths.append(path)\n",
    "        else:\n",
    "            print(\"Error: File does not exist. Please check the path and try again.\")\n",
    "\n",
    "    if not hcs_paths:\n",
    "        print(\"Error: No valid HCS files provided. Exiting.\")\n",
    "    else:\n",
    "        for hcs_path in hcs_paths:\n",
    "            hcs = process_hcs_file(hcs_path)\n",
    "            if hcs is not None:\n",
    "                all_hcs_data.append(hcs)\n",
    "                print(f\"✅ Successfully processed: {hcs_path}\")\n",
    "        if all_hcs_data:\n",
    "            hcs_df = pd.concat(all_hcs_data, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error: No HCS data successfully processed. Exiting.\")\n",
    "            exit()\n",
    "else:\n",
    "    hcs_path = input(\"Please enter the path to your HCS file (html/xls/csv): \")\n",
    "    if not os.path.exists(hcs_path):\n",
    "        print(\"Error: File does not exist. Please check the path and try again.\")\n",
    "    else:\n",
    "        hcs_df = process_hcs_file(hcs_path)\n",
    "        if hcs_df is None:\n",
    "            print(\"Error: HCS file processing failed. Exiting.\")\n",
    "            exit()\n",
    "        print(f\"✅ Successfully processed: {hcs_path}\")\n",
    "\n",
    "# --- Name Matching ---\n",
    "hcs_name_col = 'CRDH_NAME'  # Replace with actual column name if different\n",
    "\n",
    "# Normalize names\n",
    "hcs_df['norm'] = hcs_df[hcs_name_col].apply(normalize_name)\n",
    "extracted_df['norm'] = extracted_df['EXTRACTED_NAME'].apply(normalize_name)\n",
    "\n",
    "# Create matching keys\n",
    "hcs_df['match_key'] = hcs_df['norm'].apply(lambda x: f\"{x['first']}|{x['middle']}|{x['surname']}\")\n",
    "extracted_df['match_key'] = extracted_df['norm'].apply(lambda x: f\"{x['first']}|{x['middle']}|{x['surname']}\")\n",
    "\n",
    "# Use extracted_df as base (smaller set)\n",
    "matched_df = extracted_df.merge(\n",
    "    hcs_df,\n",
    "    how='left',\n",
    "    on='match_key',\n",
    "    suffixes=('_extracted', '_hcs')\n",
    ")\n",
    "\n",
    "# Add extracted name to matched rows\n",
    "matched_df['MATCHED_EXTRACTED_NAME'] = matched_df['EXTRACTED_NAME']\n",
    "\n",
    "# Clean up: keep only necessary columns and remove unmatched rows\n",
    "final_df = matched_df.dropna(subset=[hcs_name_col])  # Drop rows where HCS data wasn't matched\n",
    "final_df = final_df.drop(columns=['norm_extracted', 'norm_hcs', 'match_key'])\n",
    "\n",
    "# Results\n",
    "print(f\"\\nOriginal HCS rows: {len(hcs_df)}\")\n",
    "print(f\"Extracted names: {len(extracted_df)}\")\n",
    "print(f\"Final matched rows: {len(final_df)}\")\n",
    "print(\"\\nFirst few rows of final DataFrame:\")\n",
    "print(final_df.head())\n",
    "\n",
    "# Optionally save to CSV\n",
    "save = input(\"Would you like to save the results to a CSV file? (yes/no): \").lower().strip()\n",
    "if save == 'yes':\n",
    "    output_path = input(\"Enter the output file path (e.g., matched_output.csv): \")\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Results saved to {output_path}\")\n",
    "else:\n",
    "    print(\"Results not saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dde3c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of HCS rows with duplicate match_keys: 0\n",
      "Unique match_keys in HCS with duplicates: 0\n",
      "Rows in matched_df before dropping NaN: 1473\n",
      "Number of matched rows with duplicate match_keys: 16\n"
     ]
    }
   ],
   "source": [
    "# find duplication in HCS file\n",
    "# After creating hcs_df but before merging\n",
    "duplicate_hcs_names = hcs_df['match_key'].duplicated(keep=False)\n",
    "print(f\"Number of HCS rows with duplicate match_keys: {duplicate_hcs_names.sum()}\")\n",
    "print(f\"Unique match_keys in HCS with duplicates: {hcs_df[duplicate_hcs_names]['match_key'].nunique()}\")\n",
    "\n",
    "# After the merge, before final_df\n",
    "print(f\"Rows in matched_df before dropping NaN: {len(matched_df)}\")\n",
    "duplicate_matches = matched_df['match_key'].duplicated(keep=False)\n",
    "print(f\"Number of matched rows with duplicate match_keys: {duplicate_matches.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0e36e-9d60-4981-9998-6a8959475103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize a name by converting to uppercase, removing extra spaces, and splitting into components\"\"\"\n",
    "    if pd.isna(name) or name == \"NaN\":\n",
    "        return {\"first\": \"\", \"middle\": \"\", \"surname\": \"\"}\n",
    "    name = \" \".join(name.split()).upper()\n",
    "    parts = name.split()\n",
    "    if len(parts) == 1:\n",
    "        return {\"first\": parts[0], \"middle\": \"\", \"surname\": \"\"}\n",
    "    elif len(parts) == 2:\n",
    "        return {\"first\": parts[0], \"middle\": \"\", \"surname\": parts[1]}\n",
    "    else:\n",
    "        return {\"first\": parts[0], \"middle\": \" \".join(parts[1:-1]), \"surname\": parts[-1]}\n",
    "\n",
    "# --- Process Extracted Names ---\n",
    "multiple_txt_files = input(\"Do you have multiple text files for name extraction? (yes/no): \").lower().strip()\n",
    "all_extracted_names = []\n",
    "line_number = 0\n",
    "\n",
    "if multiple_txt_files == 'yes':\n",
    "    txt_paths = []\n",
    "    while True:\n",
    "        path = input(\"Please enter the path to a text file (or 'done' to finish): \")\n",
    "        if path.lower() == 'done':\n",
    "            break\n",
    "        if os.path.exists(path):\n",
    "            txt_paths.append(path)\n",
    "    for txt_path in txt_paths:\n",
    "        with open(txt_path, 'r', encoding='utf-8') as txt_file:\n",
    "            for line in txt_file:\n",
    "                line_number += 1\n",
    "                matches = re.findall(r'NPR\\s{0,4}([A-Z][A-Z.\\s]+)', line)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        all_extracted_names.append(match.strip())\n",
    "                else:\n",
    "                    all_extracted_names.append(\"NaN\")\n",
    "else:\n",
    "    txt_path = input(\"Please enter the path to your text file: \")\n",
    "    if os.path.exists(txt_path):\n",
    "        with open(txt_path, 'r', encoding='utf-8') as txt_file:\n",
    "            for line in txt_file:\n",
    "                line_number += 1\n",
    "                matches = re.findall(r'NPR\\s{0,4}([A-Z][A-Z.\\s]+)', line)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        all_extracted_names.append(match.strip())\n",
    "                else:\n",
    "                    all_extracted_names.append(\"NaN\")\n",
    "\n",
    "extracted_df = pd.DataFrame({'EXTRACTED_NAME': all_extracted_names})\n",
    "\n",
    "# --- Process HCS Files ---\n",
    "multiple_hcs_files = input(\"Do you have multiple HCS files to process? (yes/no): \").lower().strip()\n",
    "all_hcs_data = []\n",
    "\n",
    "if multiple_hcs_files == 'yes':\n",
    "    hcs_paths = []\n",
    "    while True:\n",
    "        path = input(\"Please enter the path to an HCS file (or 'done' to finish): \")\n",
    "        if path.lower() == 'done':\n",
    "            break\n",
    "        if os.path.exists(path):\n",
    "            hcs_paths.append(path)\n",
    "    for hcs_path in hcs_paths:\n",
    "        with open(hcs_path, 'r', encoding='utf-8') as file:\n",
    "            hcs_list = pd.read_html(file.read())\n",
    "            hcs = hcs_list[0]\n",
    "            hcs.columns = hcs.iloc[0]\n",
    "            hcs = hcs[1:].reset_index(drop=True)\n",
    "            all_hcs_data.append(hcs)\n",
    "    hcs_df = pd.concat(all_hcs_data, ignore_index=True)\n",
    "else:\n",
    "    hcs_path = input(\"Please enter the path to your HCS file: \")\n",
    "    if os.path.exists(hcs_path):\n",
    "        with open(hcs_path, 'r', encoding='utf-8') as file:\n",
    "            hcs_list = pd.read_html(file.read())\n",
    "            hcs_df = hcs_list[0]\n",
    "            hcs_df.columns = hcs_df.iloc[0]\n",
    "            hcs_df = hcs_df[1:].reset_index(drop=True)\n",
    "\n",
    "# --- Name Matching ---\n",
    "hcs_name_col = 'E_NAME'  # HCS Name (E_Name)\n",
    "account_col = 'ACCOUNT'  # ATM_ACCT\n",
    "pan_col = 'PAN'  # CARD_NUMBER (adjust if different)\n",
    "card_code_col = 'CAR_CODE'  # CARD_ID (adjust if different)\n",
    "expiry_date_col = 'EXPIRYDATE'  # EXPIR_DATE (adjust if different)\n",
    "\n",
    "# Normalize names\n",
    "hcs_df['norm'] = hcs_df[hcs_name_col].apply(normalize_name)\n",
    "extracted_df['norm'] = extracted_df['EXTRACTED_NAME'].apply(normalize_name)\n",
    "\n",
    "# Create matching keys\n",
    "hcs_df['match_key'] = hcs_df['norm'].apply(lambda x: f\"{x['first']}|{x['middle']}|{x['surname']}\")\n",
    "extracted_df['match_key'] = extracted_df['norm'].apply(lambda x: f\"{x['first']}|{x['middle']}|{x['surname']}\")\n",
    "\n",
    "# Merge with extracted_df as base\n",
    "matched_df = extracted_df.merge(\n",
    "    hcs_df,\n",
    "    how='left',\n",
    "    on='match_key',\n",
    "    suffixes=('_extracted', '_hcs')\n",
    ")\n",
    "matched_df['MATCHED_EXTRACTED_NAME'] = matched_df['EXTRACTED_NAME']\n",
    "\n",
    "# Keep only rows with HCS matches\n",
    "matched_df = matched_df.dropna(subset=[hcs_name_col])\n",
    "\n",
    "# --- Deduplication and Splitting ---\n",
    "duplicate_mask = matched_df.duplicated(subset=['EXTRACTED_NAME', account_col], keep=False)\n",
    "duplicate_df = matched_df[duplicate_mask].drop(columns=['norm_extracted', 'norm_hcs', 'match_key'])\n",
    "unique_df = matched_df[~duplicate_mask].drop(columns=['norm_extracted', 'norm_hcs', 'match_key'])\n",
    "\n",
    "# --- Compare E_Name and EXTRACTED_NAME in unique_df ---\n",
    "def check_length_match(row):\n",
    "    hcs_name = str(row[hcs_name_col]) if pd.notna(row[hcs_name_col]) else \"\"\n",
    "    extracted_name = str(row['EXTRACTED_NAME']) if pd.notna(row['EXTRACTED_NAME']) else \"\"\n",
    "    return len(hcs_name) == len(extracted_name)\n",
    "\n",
    "unique_df['length_match'] = unique_df.apply(check_length_match, axis=1)\n",
    "mismatched_df = unique_df[~unique_df['length_match']].drop(columns=['length_match'])\n",
    "matched_unique_df = unique_df[unique_df['length_match']].drop(columns=['length_match'])\n",
    "\n",
    "# --- Generate Excel Output for mismatched_df ---\n",
    "output_columns = ['ISSTYPE', 'CARD_NUMBER', 'CRDH_NAME', 'ATM_ACCT', 'ISS_DATE', 'EXPIR_DATE', 'CARD_ID']\n",
    "\n",
    "# Initialize output_df with the same number of rows as mismatched_df\n",
    "output_df = pd.DataFrame(index=mismatched_df.index, columns=output_columns)\n",
    "\n",
    "# Populate values\n",
    "output_df['ISSTYPE'] = 'NEW'  # Changed to uppercase\n",
    "output_df['CARD_NUMBER'] = mismatched_df[pan_col]\n",
    "output_df['CRDH_NAME'] = mismatched_df['EXTRACTED_NAME']\n",
    "output_df['ATM_ACCT'] = mismatched_df[account_col]\n",
    "output_df['EXPIR_DATE'] = pd.to_datetime(mismatched_df[expiry_date_col]).dt.strftime('%Y-%m-%d')\n",
    "output_df['ISS_DATE'] = (pd.to_datetime(mismatched_df[expiry_date_col]) - timedelta(days=4*365)).dt.strftime('%Y-%m-%d')\n",
    "output_df['CARD_ID'] = mismatched_df.get(card_code_col, '')\n",
    "\n",
    "# --- Output Handling ---\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "output_dir = input(\"Enter output directory (press Enter for current directory): \").strip() or os.getcwd()\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "duplicate_path = os.path.join(output_dir, 'duplicate_matches.csv')\n",
    "mismatched_csv_path = os.path.join(output_dir, 'mismatched_lengths.csv')\n",
    "matched_unique_path = os.path.join(output_dir, 'matched_unique.csv')\n",
    "excel_path = os.path.join(output_dir, 'mismatched_output.xlsx')\n",
    "\n",
    "# Results\n",
    "print(f\"\\nOriginal HCS rows: {len(hcs_df)}\")\n",
    "print(f\"Extracted names: {len(extracted_df)}\")\n",
    "print(f\"Matched rows before splitting: {len(matched_df)}\")\n",
    "print(f\"Rows with duplicate ACCOUNT and EXTRACTED_NAME: {len(duplicate_df)}\")\n",
    "print(f\"Unique or non-duplicate rows: {len(unique_df)}\")\n",
    "print(f\"Rows with mismatched lengths in unique_df: {len(mismatched_df)}\")\n",
    "print(f\"Rows with matched lengths in unique_df: {len(matched_unique_df)}\")\n",
    "print(\"\\nFirst few rows of Excel output DataFrame:\")\n",
    "print(output_df.head())\n",
    "\n",
    "# Save files with error handling\n",
    "try:\n",
    "    output_df.to_excel(excel_path, index=False)\n",
    "    duplicate_df.to_csv(duplicate_path, index=False)\n",
    "    mismatched_df.to_csv(mismatched_csv_path, index=False)\n",
    "    matched_unique_df.to_csv(matched_unique_path, index=False)\n",
    "    print(f\"✅ Results saved to:\\n- {excel_path}\\n- {duplicate_path}\\n- {mismatched_csv_path}\\n- {matched_unique_path}\")\n",
    "except PermissionError as e:\n",
    "    print(f\"❌ PermissionError: {e}\")\n",
    "    print(\"Please close any open files or run the script with appropriate permissions.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving files: {e}\")\n",
    "    er"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
